{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = input_data.read_data_sets('data/fashion',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (images) shape: (55000, 784)\n",
      "Training set (labels) shape: (55000, 10)\n",
      "Test set (images) shape: (10000, 784)\n",
      "Test set (labels) shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of training set\n",
    "print(\"Training set (images) shape: {shape}\".format(shape=data.train.images.shape))\n",
    "print(\"Training set (labels) shape: {shape}\".format(shape=data.train.labels.shape))\n",
    "\n",
    "# Shapes of test set\n",
    "print(\"Test set (images) shape: {shape}\".format(shape=data.test.images.shape))\n",
    "print(\"Test set (labels) shape: {shape}\".format(shape=data.test.labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    " 0: 'T-shirt/top',\n",
    " 1: 'Trouser',\n",
    " 2: 'Pullover',\n",
    " 3: 'Dress',\n",
    " 4: 'Coat',\n",
    " 5: 'Sandal',\n",
    " 6: 'Shirt',\n",
    " 7: 'Sneaker',\n",
    " 8: 'Bag',\n",
    " 9: 'Ankle boot',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'(Label: Ankle boot)')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATwAAACuCAYAAACr3LH6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bd1f5d8e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "curr_img = np.reshape(data.train.images[0], (28,28))\n",
    "curr_lbl = np.argmax(data.train.labels[0,:])\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "curr_img = np.reshape(data.test.images[0], (28,28))\n",
    "curr_lbl = np.argmax(data.test.labels[0,:])\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00392157,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.17254902,\n",
       "        0.49803925,  0.71372551,  0.72549021,  0.63137257,  0.47058827,\n",
       "        0.21568629,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.16470589,  0.77647066,  0.98431379,  1.        ,  0.98431379,\n",
       "        0.97647065,  0.96862751,  1.        ,  0.98823535,  0.83921576,\n",
       "        0.3921569 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.00784314,  0.        ,  0.        ,  0.91372555,  0.98823535,\n",
       "        0.92941183,  0.93725497,  0.91764712,  0.92941183,  0.92156869,\n",
       "        0.92941183,  0.92941183,  0.99607849,  0.89019614,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00392157,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00784314,  0.        ,  0.        ,\n",
       "        0.0627451 ,  0.82352948,  0.88235301,  0.84313732,  0.68627453,\n",
       "        0.85098046,  0.84705889,  0.75686282,  0.76862752,  0.88627458,\n",
       "        0.86666673,  0.81960791,  0.19607845,  0.        ,  0.        ,\n",
       "        0.00784314,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00784314,\n",
       "        0.        ,  0.        ,  0.78039223,  0.89803928,  0.90980399,\n",
       "        0.90196085,  0.96078438,  0.80000007,  0.8588236 ,  0.99215692,\n",
       "        0.96078438,  0.81176478,  0.76078439,  0.87450987,  0.90588242,\n",
       "        0.92549026,  0.92156869,  0.        ,  0.        ,  0.01176471,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00392157,  0.        ,  0.53725493,\n",
       "        0.92156869,  0.80000007,  0.81960791,  0.78823537,  0.81960791,\n",
       "        0.91764712,  0.74509805,  0.91764712,  0.85490203,  0.84313732,\n",
       "        0.9333334 ,  0.93725497,  0.80000007,  0.74117649,  0.87843144,\n",
       "        0.60392159,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.76078439,  0.78823537,  0.7843138 ,\n",
       "        0.81960791,  0.79215693,  0.75686282,  0.80392164,  0.76078439,\n",
       "        0.71764708,  0.85490203,  0.90588242,  0.77254909,  0.67450982,\n",
       "        0.70980394,  0.75686282,  0.80392164,  0.78039223,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01176471,\n",
       "        0.83137262,  0.7960785 ,  0.73725492,  0.74117649,  0.76862752,\n",
       "        0.77647066,  0.77647066,  0.78823537,  0.76862752,  0.85098046,\n",
       "        0.7019608 ,  0.65490198,  0.71764708,  0.85098046,  0.77254909,\n",
       "        0.79215693,  0.8588236 ,  0.11764707,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.13333334,  0.88235301,  0.7843138 ,\n",
       "        0.76078439,  0.74509805,  0.73725492,  0.75294125,  0.76862752,\n",
       "        0.75294125,  0.66666669,  0.79215693,  0.74509805,  0.78823537,\n",
       "        0.76470596,  0.7843138 ,  0.78823537,  0.81960791,  0.89019614,\n",
       "        0.19607845,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.26666668,  0.88235301,  0.82352948,  0.82745105,  0.77647066,\n",
       "        0.75294125,  0.76862752,  0.80000007,  0.76862752,  0.70980394,\n",
       "        0.83137262,  0.77254909,  0.76470596,  0.75294125,  0.80784321,\n",
       "        0.86274517,  0.82352948,  0.89803928,  0.36470589,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.43529415,  0.87450987,\n",
       "        0.89019614,  0.99215692,  0.81960791,  0.76862752,  0.80000007,\n",
       "        0.82745105,  0.80784321,  0.71764708,  0.84705889,  0.80784321,\n",
       "        0.82352948,  0.7960785 ,  0.84313732,  0.95686281,  0.87843144,\n",
       "        0.89019614,  0.58823532,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.54509807,  0.88235301,  0.87843144,  1.        ,\n",
       "        0.79215693,  0.80784321,  0.83137262,  0.81960791,  0.82745105,\n",
       "        0.74509805,  0.83529419,  0.79215693,  0.81176478,  0.80784321,\n",
       "        0.8705883 ,  1.        ,  0.90196085,  0.86274517,  0.74509805,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.70588237,\n",
       "        0.88627458,  0.87843144,  1.        ,  0.78039223,  0.80000007,\n",
       "        0.81176478,  0.83921576,  0.83921576,  0.74509805,  0.84705889,\n",
       "        0.80784321,  0.7960785 ,  0.80392164,  0.8588236 ,  0.95294124,\n",
       "        0.87843144,  0.83921576,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88235301,  0.87450987,  0.89411771,\n",
       "        0.99607849,  0.81960791,  0.80784321,  0.81568635,  0.83529419,\n",
       "        0.82352948,  0.74901962,  0.84313732,  0.81176478,  0.80000007,\n",
       "        0.81568635,  0.82745105,  0.97647065,  0.88627458,  0.83921576,\n",
       "        1.        ,  0.14901961,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.98039222,  0.90980399,  0.94117653,  0.93725497,  0.82745105,\n",
       "        0.7960785 ,  0.81960791,  0.80392164,  0.82745105,  0.77254909,\n",
       "        0.84313732,  0.81568635,  0.81568635,  0.83921576,  0.83529419,\n",
       "        0.93725497,  0.90588242,  0.8588236 ,  1.        ,  0.31764707,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.97254908,  0.92549026,\n",
       "        0.96862751,  0.94117653,  0.7960785 ,  0.7843138 ,  0.81568635,\n",
       "        0.80784321,  0.83921576,  0.75686282,  0.83529419,  0.83137262,\n",
       "        0.81568635,  0.83137262,  0.82745105,  0.95294124,  0.94901967,\n",
       "        0.88235301,  0.99607849,  0.25882354,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.96862751,  0.90196085,  0.98823535,  0.88627458,\n",
       "        0.78039223,  0.82745105,  0.79215693,  0.82745105,  0.83529419,\n",
       "        0.71372551,  0.83529419,  0.83137262,  0.80784321,  0.79215693,\n",
       "        0.8588236 ,  0.81176478,  0.96862751,  0.8705883 ,  0.92941183,\n",
       "        0.40784317,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.03921569,  0.95686281,\n",
       "        0.8588236 ,  0.98039222,  0.80392164,  0.78039223,  0.81960791,\n",
       "        0.79215693,  0.81960791,  0.82745105,  0.74117649,  0.83921576,\n",
       "        0.80784321,  0.82352948,  0.7843138 ,  0.83137262,  0.60392159,\n",
       "        0.94117653,  0.81568635,  0.8588236 ,  0.54901963,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.08235294,  1.        ,  0.8705883 ,  0.9333334 ,\n",
       "        0.72156864,  0.82352948,  0.75294125,  0.80784321,  0.81960791,\n",
       "        0.82352948,  0.74117649,  0.83529419,  0.82745105,  0.81960791,\n",
       "        0.75294125,  0.89411771,  0.60784316,  0.88627458,  0.9333334 ,\n",
       "        0.9450981 ,  0.65098041,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.14509805,\n",
       "        0.96078438,  0.88627458,  0.9450981 ,  0.58823532,  0.77254909,\n",
       "        0.74117649,  0.80000007,  0.81960791,  0.82352948,  0.71764708,\n",
       "        0.83529419,  0.83529419,  0.78823537,  0.72156864,  0.84313732,\n",
       "        0.57254905,  0.84705889,  0.92549026,  0.88235301,  0.60392159,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.227451  ,  0.93725497,  0.89019614,\n",
       "        1.        ,  0.61960787,  0.75686282,  0.76470596,  0.80000007,\n",
       "        0.81960791,  0.83529419,  0.70588237,  0.81176478,  0.85098046,\n",
       "        0.78039223,  0.76078439,  0.82745105,  0.61960787,  0.8588236 ,\n",
       "        0.92549026,  0.84705889,  0.59215689,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.26666668,  0.91372555,  0.88627458,  0.95294124,  0.54509807,\n",
       "        0.7843138 ,  0.75686282,  0.80392164,  0.82352948,  0.81568635,\n",
       "        0.70588237,  0.80392164,  0.83137262,  0.7960785 ,  0.76862752,\n",
       "        0.84705889,  0.6156863 ,  0.7019608 ,  1.        ,  0.84705889,\n",
       "        0.60784316,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.31764707,  0.88235301,\n",
       "        0.87843144,  0.82745105,  0.5411765 ,  0.8588236 ,  0.72549021,\n",
       "        0.78823537,  0.83529419,  0.81176478,  0.77254909,  0.88627458,\n",
       "        0.83137262,  0.7843138 ,  0.74509805,  0.84313732,  0.71764708,\n",
       "        0.35294119,  1.        ,  0.82745105,  0.57647061,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.35686275,  0.82352948,  0.90196085,  0.61960787,\n",
       "        0.44705886,  0.80392164,  0.73333335,  0.81568635,  0.81960791,\n",
       "        0.80784321,  0.75686282,  0.82352948,  0.82745105,  0.80000007,\n",
       "        0.76470596,  0.80000007,  0.70980394,  0.09019608,  1.        ,\n",
       "        0.83529419,  0.61960787,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.34117648,\n",
       "        0.80392164,  0.90980399,  0.42745101,  0.64313728,  1.        ,\n",
       "        0.83921576,  0.87843144,  0.8705883 ,  0.82352948,  0.77254909,\n",
       "        0.83921576,  0.88235301,  0.8705883 ,  0.82745105,  0.86274517,\n",
       "        0.85098046,  0.        ,  0.91764712,  0.84705889,  0.66274512,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.36078432,  0.83529419,  0.90980399,\n",
       "        0.57254905,  0.01960784,  0.52549022,  0.59215689,  0.63529414,\n",
       "        0.66666669,  0.71764708,  0.71372551,  0.64313728,  0.65098041,\n",
       "        0.69803923,  0.63529414,  0.61176473,  0.38431376,  0.        ,\n",
       "        0.94117653,  0.88235301,  0.82352948,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.16862746,  0.64313728,  0.80784321,  0.5529412 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.49803925,  0.49019611,\n",
       "        0.29803923,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data.train.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(data.train.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training and testing image\n",
    "train_X = data.train.images.reshape(-1, 28, 28, 1)\n",
    "test_X = data.test.images.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = data.train.labels\n",
    "test_y = data.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 10), (10000, 10))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iters = 200 \n",
    "learning_rate = 0.001 \n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data input (img shape: 28*28)\n",
    "n_input = 28\n",
    "\n",
    "# MNIST total classes (0-9 digits)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both placeholders are of type float\n",
    "x = tf.placeholder(\"float\", [None, 28,28,1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x) \n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'wc1': tf.get_variable('W0', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'wc2': tf.get_variable('W1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'wd1': tf.get_variable('W3', shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "    'out': tf.get_variable('W6', shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'out': tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases):  \n",
    "\n",
    "    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Output, class prediction\n",
    "    # finally we multiply the fully connected layer with the weights and add a bias term. \n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-5afe8b1429e1>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = conv_net(x, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "#calculate accuracy across all the given images and average them out. \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 0.312522, Training Accuracy= 0.87500\n",
      "Optimization Finished!\n",
      "Testing Accuracy:"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
    "    for i in range(training_iters):\n",
    "        for batch in range(len(train_X)//batch_size):\n",
    "            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
    "            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n",
    "            # Run optimization op (backprop).\n",
    "                # Calculate batch loss and accuracy\n",
    "            opt = sess.run(optimizer, feed_dict={x: batch_x,\n",
    "                                                              y: batch_y})\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y})\n",
    "        print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for all 10000 mnist test images\n",
    "        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n",
    "        train_loss.append(loss)\n",
    "        test_loss.append(valid_loss)\n",
    "        train_accuracy.append(acc)\n",
    "        test_accuracy.append(test_acc)\n",
    "        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
